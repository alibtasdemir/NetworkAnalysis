{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\baran\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import io\n",
    "from sklearn import svm, base, feature_selection, linear_model\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve,auc,classification_report,f1_score,accuracy_score,roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import subprocess\n",
    "from os import listdir\n",
    "#to visualize\n",
    "from sklearn.externals.six import StringIO\n",
    "# import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_random_soc-firm-hi-tech_final\n",
    "# Train and test paths\n",
    "train_data_path = \"__Random_Features\\\\features_random_soc-firm-hi-tech_final.csv\"\n",
    "test_data_path = \"__Real_Features\\\\soc-firm-hi-tech_final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "df = pd.read_csv(train_data_path).drop(columns='Unnamed: 0')\n",
    "model_test = pd.read_csv(test_data_path).drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train. \n",
    "# X: Independent values\n",
    "# y: Target values (Categories)\n",
    "def prepareData(df, normalize=False):\n",
    "    X = np.array(df.drop(['Name', 'Categories'], 1))\n",
    "    if normalize:\n",
    "        return (X - X.mean()) / (X.max() - X.min()), np.array(df['Categories'])\n",
    "    return X, np.array(df['Categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doSVMcvPrediction(Xin,yin,Xtest,ytest,modType):\n",
    "    #input: training/test data and labels, model type\n",
    "    #supported models: SVM (l1 and l2), AdaBoost, decision tree, and random forest\n",
    "    #train with 5-fold cross validation, then test once using test (holdout) data\n",
    "    #once the best estimator is chosen here, train on the entire dataset (in + test) outside this function\n",
    "    #output: training accuracy, gereralization accuracy, feature weights/importances, classifier, \n",
    "    #  classification report, training f1-score and generalization f1-score\n",
    "    nfolds = 5\n",
    "    cv = StratifiedKFold(n_splits=nfolds, shuffle=True)\n",
    "    #l1 penalty enforces sparsity in weights, only available for linear SVM classifier\n",
    "    if modType in ('SVM-L2','svm-l2'):\n",
    "        clasf = svm.LinearSVC(loss='squared_hinge', penalty='l2', tol=.001, dual=False, class_weight='balanced')\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'C' : [0.05, 0.1, 0.5, 1, 5, 10, 500, 1000]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            # scoring='roc_auc',\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('SVM-L1','svm-l1'):\n",
    "        clasf = svm.LinearSVC(loss='squared_hinge', penalty='l1', tol=.001, dual=False, class_weight='balanced')\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'C' : [0.05, 0.1, 0.5, 1, 5, 10, 500, 1000]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "    \n",
    "    #decision tree classifiers\n",
    "    elif modType in ('ada','adaboost','adaboost-tree'):\n",
    "        clasf = AdaBoostClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_estimators' : [5,10,25,50,100],\n",
    "            'learning_rate' : [0.1,0.3,0.5]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('dtree','decision-tree'):\n",
    "        clasf = DecisionTreeClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'splitter' : ['best'],\n",
    "            'criterion' : ['entropy','gini'],\n",
    "            'max_features' : [0.2,'sqrt',1.],\n",
    "            'max_depth' : [2,4], \n",
    "            'class_weight' : ['balanced'], \n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('rf','random-forest'):\n",
    "        clasf = RandomForestClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_estimators' : [5,10,25,50,100],\n",
    "            'criterion' : ['entropy','gini'],\n",
    "            'max_features' : [0.2,'sqrt',1.],\n",
    "            'max_depth' : [2,4], \n",
    "            'class_weight' : ['balanced'], \n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "    \n",
    "    elif modType in ('knn', 'kneighbors'):\n",
    "        clasf = KNeighborsClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 11, 13, 15, 19],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "            }, verbose=0, refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    cvclasf.fit(Xin,yin)\n",
    "    bclasf = cvclasf.best_estimator_\n",
    "    print(\"%s %d-fold CV params: %s\" % (modType,nfolds,cvclasf.best_params_))\n",
    "    \n",
    "    if modType in ('ada','adaboost-tree','dtree','decision-tree','rf','random-forest'):\n",
    "        w = bclasf.feature_importances_\n",
    "    elif modType in ('SVM-L1','svm-l1','SVM-L2','svm-l2'):\n",
    "        w = bclasf.coef_\n",
    "    elif modType in ('knn', 'kneighbors'):\n",
    "        w = np.zeros(2)\n",
    "    \n",
    "    bclasf.fit(Xin,yin)\n",
    "    y_train_pred = bclasf.predict(Xin)\n",
    "    acTrain = accuracy_score(yin,y_train_pred)\n",
    "    f1Train = f1_score(yin,y_train_pred,average=\"weighted\")\n",
    "    \n",
    "    y_pred = bclasf.predict(Xtest)\n",
    "    report = classification_report(ytest, y_pred)\n",
    "    acGeneral = accuracy_score(ytest, y_pred)\n",
    "    f1Gen = f1_score(ytest,y_pred,average=\"weighted\")\n",
    "\n",
    "    return(acTrain,np.squeeze(w),bclasf,report,(acTrain,acGeneral),(f1Train,f1Gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(df_train, df_testing, modelType):\n",
    "    X, y = prepareData(df_train)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "    \n",
    "    RGfamilies = [\"CL\", \"CNFG\", \"GNP\", \"PA\"]\n",
    "    \n",
    "    # Preprocess\n",
    "    scaler1=StandardScaler()\n",
    "    X_train = scaler1.fit_transform(X_train.astype(np.double))\n",
    "    X_test = scaler1.transform(X_test.astype(np.double))\n",
    "    \n",
    "    \n",
    "    score,optWeights,clasf,rep,accs,f1s = doSVMcvPrediction(X_train, y_train, X_test, y_test, modelType)\n",
    "    \n",
    "    print(rep)\n",
    "    print()\n",
    "    \n",
    "    test_measure = df_testing.drop(columns=['Name'])\n",
    "    x = np.array(test_measure.values)\n",
    "    \n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "    X = scaler.fit_transform(X.astype(np.double))\n",
    "    x = scaler.transform(x.astype(np.double))\n",
    "    \n",
    "    \n",
    "    clasf.fit(X,y)\n",
    "    novelRG = clasf.predict(x)\n",
    "    print(\"\\nPrediction is %s\\n\"%(novelRG))\n",
    "    \n",
    "    Fcol = df.drop(['Name', 'Categories'], 1).columns\n",
    "    \n",
    "    if modelType in ('SVM-L1','SVM-L2'):\n",
    "        #TODO: print statements when useSpectral\n",
    "        print(\"SVM feature weights:\")\n",
    "        print(np.column_stack((clasf.coef_.T,Fcol)))\n",
    "        sc = x.dot(clasf.coef_.T) + clasf.intercept_\n",
    "        print(\"scores:\")\n",
    "        print(np.vstack((RGfamilies,sc)))\n",
    "    elif modelType in ('adaboost-tree'):\n",
    "        print(\"Adaboost feature weights:\")\n",
    "        print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "        # print \"prediction probabilities:\"\n",
    "        # print np.vstack((RGfamilies,clasf.predict_proba(x)))\n",
    "        print(\"decision function:\")\n",
    "        print(np.vstack((RGfamilies,clasf.decision_function(x))))\n",
    "        #also the actual tree?\n",
    "    elif modelType in ('decision-tree'):\n",
    "        print(\"Decision tree feature weights:\")\n",
    "        print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "        print(\"prediction probabilities:\")\n",
    "        print(np.vstack((RGfamilies,clasf.predict_proba(x))))\n",
    "        #also the actual tree\n",
    "        #Fcol2 = [s.replace('n3','H').replace('n4','F') for s in Fcol]\n",
    "        #writeTree(clasf,Fcol2,graphFolder[:-1]+'_dTree.pdf')\n",
    "    elif modelType in ('random-forest'):\n",
    "        print(\"Random Forest feature weights:\")\n",
    "        print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "        print(\"prediction probabilities:\")\n",
    "        print(np.vstack((RGfamilies,clasf.predict_proba(x))))\n",
    "    elif modelType in ('knn'):\n",
    "        print(\"K Nearest Neighbors feature weights:\")\n",
    "        #print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "        print(\"prediction probabilities:\")\n",
    "        print(np.vstack((RGfamilies,clasf.predict_proba(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testModel(df, model_test, \"SVM-L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testModel(df, model_test, \"SVM-L2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testModel(df, model_test, \"adaboost-tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testModel(df, model_test, \"decision-tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testModel(df, model_test, \"random-forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn 5-fold CV params: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CL       1.00      1.00      1.00        50\n",
      "        CNFG       1.00      1.00      1.00        50\n",
      "         GNP       1.00      1.00      1.00        50\n",
      "          PA       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "\n",
      "\n",
      "Prediction is ['CNFG']\n",
      "\n",
      "K Nearest Neighbors feature weights:\n",
      "prediction probabilities:\n",
      "[['CL' 'CNFG' 'GNP' 'PA']\n",
      " ['0.0' '1.0' '0.0' '0.0']]\n"
     ]
    }
   ],
   "source": [
    "testModel(df, model_test, \"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
